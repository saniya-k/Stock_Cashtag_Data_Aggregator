{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tweets related to hashtag/cashtag \n",
    "\n",
    "This module can pull data from twitter using tweepy . You must have access to twitter's developer API as it uses your credentials to connect & access data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date,timedelta\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get connection to API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authorized(consumer_key,consumer_secret,access_token,access_token_secret):\n",
    "    '''    \n",
    "    This function helps to establish connection to twitter api via tweepy.\n",
    "    \n",
    "    Parameters:\n",
    "    consumer_key : api key from twitter enter by user\n",
    "    consumer_secret : api secret from twitter enter by user\n",
    "    access_token : access token from twitter enter by user\n",
    "    access_token_secret: access token secret from twitter enter by user\n",
    "    \n",
    "    Returns: \n",
    "    Tweepy api object which helps in connecting to twitter\n",
    "    '''\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True) #waits on rate limit (15 mins)\n",
    "    return api\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get tweets related to hashtag\n",
    "\n",
    "Twitter has a rate limit of 900 request / 15 min PER USER AUTH &  300 request / 15 min PER APP AUTH. And it only allows pulling data for past 1 week. We are pulling tweets made in the past 2 days for each company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(api,tag,num_tweets=10):\n",
    "    '''\n",
    "    This function gets tweets related to a cashtag made in the past two days.\n",
    "    \n",
    "    Parameters:\n",
    "    api : tweepy api object which helps in connecting to twitter\n",
    "    tag : cashtag\n",
    "    num_tweets : Number of tweets to be scrapped. If this exceeds 300, the API will try to get all the tweets made in the past two days.\n",
    "    \n",
    "    Returns: \n",
    "    A list of tweets for the corresponding cashtag.\n",
    "    '''\n",
    "    today = date.today()\n",
    "    text_query = tag \n",
    "    try:\n",
    "        if num_tweets>300:\n",
    "            tweets = tweepy.Cursor(api.search,q=text_query,since=today-timedelta(days=2), until=today).items()\n",
    "        else:\n",
    "            tweets = tweepy.Cursor(api.search,q=text_query,since=today-timedelta(days=2), until=today).items(num_tweets)\n",
    "        # Pulling information from tweets iterable object\n",
    "        tweets_list = [[tweet.created_at, tweet.id, tweet.text] for tweet in tweets]\n",
    "        # Creation of dataframe from tweets list\n",
    "        # Add or remove columns as you remove tweet information\n",
    "        tweets_df = pd.DataFrame(tweets_list)\n",
    "        tweets_df.rename(columns={0: 'Timestamp',1: 'Tweet ID',2:'Tweet'}, inplace=True)\n",
    "        tweets_df.insert(0, 'Hashtag', tag)\n",
    "    except tweepy.TweepError as te:\n",
    "        print(te.reason)\n",
    "        return None \n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check connection to API \n",
    "For this you have to edit the file authentication.txt, add ur credentitals to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tweepy.api.API object at 0x0000018DDF4B40A0>\n"
     ]
    }
   ],
   "source": [
    "#get from twitter after creating dev account\n",
    "f=open(\"authentication.txt\",\"r\")\n",
    "lines=f.readlines()\n",
    "api_key=lines[0].strip() \n",
    "api_secret_key=lines[1].strip() \n",
    "access_token=lines[2].strip() \n",
    "access_token_secret=lines[3].strip() \n",
    "api = get_authorized(api_key,api_secret_key,access_token,access_token_secret)\n",
    "print(api)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Tech Companies list file and get tweets for corresponding tech company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from twitter ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 831\n",
      "Rate limit reached. Sleeping for: 829\n",
      "Rate limit reached. Sleeping for: 824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to send request: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Skipped Apple Inc. due to exception, will retry later\n",
      "Done... \n",
      " Some of the companies are left due exception, run below code to try again: {'AAPL': 'Apple Inc.'}\n"
     ]
    }
   ],
   "source": [
    "#read file containing hashtag information\n",
    "hashtags=pd.read_csv(\"FinalList_Tech.csv\",header=0)\n",
    "related_tweets=pd.DataFrame()\n",
    "hashtags.head()\n",
    "retry_later={}\n",
    "print(\"Getting data from twitter ...\")\n",
    "for lab,row in hashtags.iterrows():\n",
    "    #function gets 10 tweets related to cashtag in the past week\n",
    "    tweets_df=get_tweets(api,'$'+row[10],2000)\n",
    "    if (tweets_df is None):\n",
    "        print(f\"Skipped {row[3]} due to exception, will retry later\")\n",
    "        retry_later[row[10]]=row[3]\n",
    "        continue\n",
    "    if not(tweets_df.empty) :\n",
    "        tweets_df.insert(0, 'Company', row[3])\n",
    "        related_tweets=related_tweets.append(tweets_df, ignore_index=True)\n",
    "print(\"Done... \")\n",
    "if retry_later:\n",
    "    print(f' Some of the companies are left due exception, run below code to try again: {retry_later}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrying the companies skipped above due to exceptions (ConnectionResetError etc)\n",
    "\n",
    "The program runs loops 3 times at max, to avoid going into an infinite loop. Also it only pulls 300 tweets for the skipped companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying for skipped ones...\n"
     ]
    }
   ],
   "source": [
    "remove_keys=[]\n",
    "num_tries=0\n",
    "wait_time =180\n",
    "while len(retry_later)>0:\n",
    "    print(\"Retrying for skipped ones...\")\n",
    "    num_tries+=1\n",
    "    if num_tries>3:\n",
    "        print(\"Max tries exceeded, exiting... \")\n",
    "        break\n",
    "    for key,val in retry_later.items():\n",
    "        time.sleep(wait_time) #sleep in case we reached rate limit earlier\n",
    "        wait_time += 1\n",
    "        tweets_df=get_tweets(api,'$'+key,300)\n",
    "        if (tweets_df is None):\n",
    "            print(f\"Skipped {key} due to exception, will retry later\")\n",
    "            continue\n",
    "        if not(tweets_df.empty) :\n",
    "            tweets_df.insert(0, 'Company', val)\n",
    "            related_tweets=related_tweets.append(tweets_df, ignore_index=True)\n",
    "            remove_keys.append(key)\n",
    "    \n",
    "    if remove_keys:\n",
    "        for k in remove_keys: del retry_later[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Teledyne Technologies</td>\n",
       "      <td>$TDY</td>\n",
       "      <td>2020-11-28 22:24:35</td>\n",
       "      <td>1332812659805196288</td>\n",
       "      <td>@investing_city $TDY $CGNX $IEX $APH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Teledyne Technologies</td>\n",
       "      <td>$TDY</td>\n",
       "      <td>2020-11-28 19:45:39</td>\n",
       "      <td>1332772663077588995</td>\n",
       "      <td>$TDY in Uptrend: 50-day Moving Average moved a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Teledyne Technologies</td>\n",
       "      <td>$TDY</td>\n",
       "      <td>2020-11-27 20:42:52</td>\n",
       "      <td>1332424674148450304</td>\n",
       "      <td>Comcast Corp $CMCSA To Reveal Significant Inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Teledyne Technologies</td>\n",
       "      <td>$TDY</td>\n",
       "      <td>2020-11-27 17:11:35</td>\n",
       "      <td>1332371504030334977</td>\n",
       "      <td>$TDY in Uptrend: 50-day Moving Average moved a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Teledyne Technologies</td>\n",
       "      <td>$TDY</td>\n",
       "      <td>2020-11-27 14:52:28</td>\n",
       "      <td>1332336496183967745</td>\n",
       "      <td>my long holdings:\\n$aapl\\n$adbe\\n$amd\\n$amzn\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Company Hashtag           Timestamp             Tweet ID  \\\n",
       "0  Teledyne Technologies    $TDY 2020-11-28 22:24:35  1332812659805196288   \n",
       "1  Teledyne Technologies    $TDY 2020-11-28 19:45:39  1332772663077588995   \n",
       "2  Teledyne Technologies    $TDY 2020-11-27 20:42:52  1332424674148450304   \n",
       "3  Teledyne Technologies    $TDY 2020-11-27 17:11:35  1332371504030334977   \n",
       "4  Teledyne Technologies    $TDY 2020-11-27 14:52:28  1332336496183967745   \n",
       "\n",
       "                                               Tweet  \n",
       "0               @investing_city $TDY $CGNX $IEX $APH  \n",
       "1  $TDY in Uptrend: 50-day Moving Average moved a...  \n",
       "2  Comcast Corp $CMCSA To Reveal Significant Inve...  \n",
       "3  $TDY in Uptrend: 50-day Moving Average moved a...  \n",
       "4  my long holdings:\\n$aapl\\n$adbe\\n$amd\\n$amzn\\n...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_tweets.to_csv(f\"Twitter_Tech_data_{date.today()}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new column number of tweets to original list of tech companies, and save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tweets_per_company=related_tweets[['Company','Tweet']].groupby('Company').agg({'Tweet':'count'}).reset_index()\n",
    "hashtags=pd.merge(hashtags,num_tweets_per_company,on='Company',how='left')\n",
    "hashtags.to_csv(f\"FinalList_Tech_{date.today()}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the entire notebook contents into a script file (.py)\n",
    "This file can be executed automatically daily if we we add the following line to the crontab using the command - <br>\n",
    "@daily python3 /Projects/scraper/getTwitterData.py <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing getTwitterData.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile getTwitterData.py\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date,timedelta\n",
    "from pprint import pprint\n",
    "\n",
    "def get_authorized(consumer_key,consumer_secret,access_token,access_token_secret):\n",
    "    '''    \n",
    "    This function helps to establish connection to twitter api via tweepy.\n",
    "    \n",
    "    Parameters:\n",
    "    consumer_key : api key from twitter enter by user\n",
    "    consumer_secret : api secret from twitter enter by user\n",
    "    access_token : access token from twitter enter by user\n",
    "    access_token_secret: access token secret from twitter enter by user\n",
    "    \n",
    "    Returns: \n",
    "    Tweepy api object which helps in connecting to twitter\n",
    "    '''\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True) #waits on rate limit (15 mins)\n",
    "    return api\n",
    "\n",
    "def get_tweets(api,tag,num_tweets=10):\n",
    "    '''\n",
    "    This function gets tweets related to a cashtag made in the past two days.\n",
    "    \n",
    "    Parameters:\n",
    "    api : tweepy api object which helps in connecting to twitter\n",
    "    tag : cashtag\n",
    "    num_tweets : Number of tweets to be scrapped. If this exceeds 300, the API will try to get all the tweets made in the past two days.\n",
    "    \n",
    "    Returns: \n",
    "    A list of tweets for the corresponding cashtag.\n",
    "    '''\n",
    "    today = date.today()\n",
    "    text_query = tag \n",
    "    try:\n",
    "        if num_tweets>300:\n",
    "            tweets = tweepy.Cursor(api.search,q=text_query,since=today-timedelta(days=2), until=today).items()\n",
    "        else:\n",
    "            tweets = tweepy.Cursor(api.search,q=text_query,since=today-timedelta(days=2), until=today).items(num_tweets)\n",
    "        # Pulling information from tweets iterable object\n",
    "        tweets_list = [[tweet.created_at, tweet.id, tweet.text] for tweet in tweets]\n",
    "        # Creation of dataframe from tweets list\n",
    "        # Add or remove columns as you remove tweet information\n",
    "        tweets_df = pd.DataFrame(tweets_list)\n",
    "        tweets_df.rename(columns={0: 'Timestamp',1: 'Tweet ID',2:'Tweet'}, inplace=True)\n",
    "        tweets_df.insert(0, 'Hashtag', tag)\n",
    "    except tweepy.TweepError as te:\n",
    "        print(te.reason)\n",
    "        return None \n",
    "    return tweets_df\n",
    "\n",
    "#get from twitter after creating dev account\n",
    "f=open(\"authentication.txt\",\"r\")\n",
    "lines=f.readlines()\n",
    "api_key=lines[0].strip() \n",
    "api_secret_key=lines[1].strip() \n",
    "access_token=lines[2].strip() \n",
    "access_token_secret=lines[3].strip() \n",
    "api = get_authorized(api_key,api_secret_key,access_token,access_token_secret)\n",
    "f.close()\n",
    "\n",
    "#read file containing hashtag information\n",
    "hashtags=pd.read_csv(\"FinalList_Tech.csv\",header=0) #file location\n",
    "related_tweets=pd.DataFrame()\n",
    "hashtags.head()\n",
    "retry_later={}\n",
    "print(\"Getting data from twitter ...\")\n",
    "for lab,row in hashtags.iterrows():\n",
    "    #function gets 10 tweets related to cashtag in the past week\n",
    "    tweets_df=get_tweets(api,'$'+row[10],2000)\n",
    "    if (tweets_df is None):\n",
    "        print(f\"Skipped {row[3]} due to exception, will retry later\")\n",
    "        retry_later[row[10]]=row[3]\n",
    "        continue\n",
    "    if not(tweets_df.empty) :\n",
    "        tweets_df.insert(0, 'Company', row[3])\n",
    "        related_tweets=related_tweets.append(tweets_df, ignore_index=True)\n",
    "print(\"Done... \")\n",
    "if retry_later:\n",
    "    print(f' Some of the companies are left due exception, run below code to try again: {retry_later}')\n",
    "    \n",
    "#Code to retry getting data for failed companies    \n",
    "remove_keys=[]\n",
    "num_tries=0\n",
    "wait_time =180\n",
    "while len(retry_later)>0:\n",
    "    print(\"Retrying for skipped ones...\")\n",
    "    num_tries+=1\n",
    "    if num_tries>3:\n",
    "        print(\"Max tries exceeded, exiting... \")\n",
    "        break\n",
    "    for key,val in retry_later.items():\n",
    "        time.sleep(wait_time) #sleep in case we reached rate limit earlier\n",
    "        wait_time += 1\n",
    "        tweets_df=get_tweets(api,'$'+key,300)\n",
    "        if (tweets_df is None):\n",
    "            print(f\"Skipped {key} due to exception, will retry later\")\n",
    "            continue\n",
    "        if not(tweets_df.empty) :\n",
    "            tweets_df.insert(0, 'Company', val)\n",
    "            related_tweets=related_tweets.append(tweets_df, ignore_index=True)\n",
    "            remove_keys.append(key)\n",
    "    \n",
    "    if remove_keys:\n",
    "        for k in remove_keys: del retry_later[k]\n",
    "            \n",
    "#Save list of tweets            \n",
    "related_tweets.to_csv(f\"Twitter_Tech_data_{date.today()}.csv\", index=False)\n",
    "\n",
    "#Update original tech list with number of tweets and save the file\n",
    "num_tweets_per_company=related_tweets[['Company','Tweet']].groupby('Company').agg({'Tweet':'count'}).reset_index()\n",
    "hashtags=pd.merge(hashtags,num_tweets_per_company,on='Company',how='left')\n",
    "hashtags.to_csv(f\"FinalList_Tech_{date.today()}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "titlepage": {
   "author": "Saniya Khan",
   "email": "sk3862@drexel.edu",
   "supervisors": [
    "Prof. Jake Williams"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
